ğŸ§  RAG Pipeline Chatbot using n8n
This project implements a Retrieval-Augmented Generation (RAG) Chatbot using n8n, the powerful open-source workflow automation platform. It combines context-aware question answering with real-time document retrieval, enabling smarter and more accurate AI-powered conversations â€” ideal for use cases like customer support, internal documentation bots, product assistants, and more.

ğŸ” What is RAG?
Retrieval-Augmented Generation (RAG) is a hybrid AI approach that enhances large language models (LLMs) by combining:

Retrieval: Fetching relevant documents from external sources (e.g., Notion, Google Docs, PDF files, APIs, or databases).

Generation: Using an LLM (like OpenAI, Gemini, or Hugging Face) to generate a response based on the retrieved context.

Unlike traditional chatbots, RAG ensures factual accuracy by grounding answers in real data.

âš™ï¸ How It Works (Using n8n)
This chatbot is built as an n8n workflow, leveraging its no-code/low-code capabilities to create a flexible and modular architecture:

Trigger Node

Accepts input from users via Webhook, Telegram, Discord, or web form.

Text Preprocessing

Optional steps to clean or reformat the user input.

Embedding & Retrieval

User query is converted into a vector using an embedding model (OpenAI, Cohere, or Google embeddings).

Search against a vector database (like Qdrant, Pinecone, or Weaviate) to retrieve the most relevant documents.

Context Construction

Top results are compiled into a prompt format with instructions for the LLM.

LLM Generation

A language model (e.g., OpenAI GPT-4, Gemini 1.5, or Hugging Face models) generates a response based on the query and retrieved context.

Response Delivery

The final answer is sent back to the user through the same communication channel.

ğŸ“¦ Features
âœ… No-code RAG implementation with full customization in n8n

ğŸ“š Easily plug in different data sources (PDFs, Notion, APIs, databases)

ğŸ§  LLM-agnostic: works with OpenAI, Gemini, Hugging Face, and more

âš¡ Real-time semantic search using vector embeddings

ğŸ”’ Private, on-premise or cloud-based deployment

ğŸ”„ Fully automated and scalable workflows

ğŸš€ Use Cases
Internal Knowledge Bots (for HR, IT, DevOps, etc.)

Customer Support Assistants

Documentation Q&A bots

Product Recommendation Assistants

Legal, Finance, or Education Q&A

ğŸ§± Tech Stack
n8n â€“ Workflow engine and orchestrator

OpenAI / Gemini / Hugging Face â€“ Language model APIs

Pinecone / Qdrant / Weaviate â€“ Vector similarity search

PostgreSQL / Supabase / Notion / Google Docs â€“ Knowledge sources

Telegram / Discord / Webhook â€“ Chatbot user interface

ğŸ“ Folder Structure (Example)
pgsql
Copy
Edit
â”œâ”€â”€ n8n-workflows/
â”‚   â”œâ”€â”€ chatbot-workflow.json
â”‚   â””â”€â”€ data-ingestion-workflow.json
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ setup_guide.md
â”œâ”€â”€ vector-db/
â”‚   â””â”€â”€ qdrant_config.json
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ chatbot_prompt_template.txt
â””â”€â”€ README.md

ğŸ› ï¸ Setup Guide
Clone the repository and install n8n.

Import the chatbot-workflow.json into n8n.

Configure:

API keys for LLMs and vector databases

Data sources (Notion, Google Docs, CSV, etc.)

Deploy and start the workflow.

Connect to your preferred chat interface.

ğŸ’¡ Tips
You can schedule data ingestion workflows in n8n to keep your knowledge base up to date.

Use n8n's built-in credential manager for secure API integration.

Try chunking large documents and indexing them for better retrieval accuracy.

ğŸ“œ License
This project is open-sourced under the MIT License.
